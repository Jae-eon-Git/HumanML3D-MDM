{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing joints:  33%|███████████████████▎                                      | 9714/29232 [03:47<08:13, 39.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] 009707.npy: cannot reshape array of size 0 into shape (0,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing joints:  38%|█████████████████████▌                                   | 11066/29232 [04:20<06:39, 45.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] 011059.npy: cannot reshape array of size 0 into shape (0,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing joints:  83%|███████████████████████████████████████████████▍         | 24332/29232 [09:33<01:59, 40.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] M009707.npy: cannot reshape array of size 0 into shape (0,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing joints:  88%|██████████████████████████████████████████████████       | 25683/29232 [10:05<01:18, 45.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] M011059.npy: cannot reshape array of size 0 into shape (0,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing joints: 100%|█████████████████████████████████████████████████████████| 29232/29232 [11:28<00:00, 42.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clips: 29232, Frames: 4117392, Duration: 3431.160000m\n",
      "Loaded reference clips 012314 for sanity check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import join as pjoin\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common.skeleton import Skeleton\n",
    "from common.quaternion import *\n",
    "from paramUtil import *\n",
    "\n",
    "# ------------------ helper functions ------------------ #\n",
    "\n",
    "def uniform_skeleton(positions, target_offset):\n",
    "    src_skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
    "    src_offset = src_skel.get_offsets_joints(torch.from_numpy(positions[0]))\n",
    "    src_offset = src_offset.numpy()\n",
    "    tgt_offset = target_offset.numpy()\n",
    "\n",
    "    # 다리 길이 비율로 스케일 맞추기\n",
    "    src_leg_len = np.abs(src_offset[l_idx1]).max() + np.abs(src_offset[l_idx2]).max()\n",
    "    tgt_leg_len = np.abs(tgt_offset[l_idx1]).max() + np.abs(tgt_offset[l_idx2]).max()\n",
    "    scale_rt = tgt_leg_len / src_leg_len\n",
    "\n",
    "    src_root_pos = positions[:, 0]\n",
    "    tgt_root_pos = src_root_pos * scale_rt\n",
    "\n",
    "    # IK -> 포즈를 quaternion으로\n",
    "    quat_params = src_skel.inverse_kinematics_np(positions, face_joint_indx)\n",
    "\n",
    "    # FK -> 타겟 스켈레톤으로 forward kinematics\n",
    "    src_skel.set_offset(target_offset)\n",
    "    new_joints = src_skel.forward_kinematics_np(quat_params, tgt_root_pos)\n",
    "    return new_joints\n",
    "\n",
    "\n",
    "def process_file(positions, feet_thre):\n",
    "    # positions: (seq_len, joints_num, 3)\n",
    "\n",
    "    # 1) 스켈레톤 통일\n",
    "    positions = uniform_skeleton(positions, tgt_offsets)\n",
    "\n",
    "    # 2) 바닥에 붙이기 (y축 최소를 0으로)\n",
    "    floor_height = positions.min(axis=0).min(axis=0)[1]\n",
    "    positions[:, :, 1] -= floor_height\n",
    "\n",
    "    # 3) XZ 원점 정렬\n",
    "    root_pos_init = positions[0]\n",
    "    root_pose_init_xz = root_pos_init[0] * np.array([1, 0, 1])\n",
    "    positions = positions - root_pose_init_xz\n",
    "\n",
    "    # 4) 초기 방향을 Z+ 로 맞추기\n",
    "    r_hip, l_hip, sdr_r, sdr_l = face_joint_indx\n",
    "    across1 = root_pos_init[r_hip] - root_pos_init[l_hip]\n",
    "    across2 = root_pos_init[sdr_r] - root_pos_init[sdr_l]\n",
    "    across = across1 + across2\n",
    "    across = across / np.sqrt((across ** 2).sum(axis=-1))[..., np.newaxis]\n",
    "\n",
    "    forward_init = np.cross(np.array([[0, 1, 0]]), across, axis=-1)\n",
    "    forward_init = forward_init / np.sqrt((forward_init ** 2).sum(axis=-1))[..., np.newaxis]\n",
    "\n",
    "    target = np.array([[0, 0, 1]])\n",
    "    root_quat_init = qbetween_np(forward_init, target)\n",
    "    root_quat_init = np.ones(positions.shape[:-1] + (4,)) * root_quat_init\n",
    "\n",
    "    positions_b = positions.copy()\n",
    "    positions = qrot_np(root_quat_init, positions)\n",
    "\n",
    "    global_positions = positions.copy()\n",
    "\n",
    "    # 5) 발 접촉 탐지\n",
    "    def foot_detect(positions, thres):\n",
    "        velfactor = np.array([thres, thres])\n",
    "\n",
    "        feet_l_x = (positions[1:, fid_l, 0] - positions[:-1, fid_l, 0]) ** 2\n",
    "        feet_l_y = (positions[1:, fid_l, 1] - positions[:-1, fid_l, 1]) ** 2\n",
    "        feet_l_z = (positions[1:, fid_l, 2] - positions[:-1, fid_l, 2]) ** 2\n",
    "        feet_l = ((feet_l_x + feet_l_y + feet_l_z) < velfactor).astype(np.float32)\n",
    "\n",
    "        feet_r_x = (positions[1:, fid_r, 0] - positions[:-1, fid_r, 0]) ** 2\n",
    "        feet_r_y = (positions[1:, fid_r, 1] - positions[:-1, fid_r, 1]) ** 2\n",
    "        feet_r_z = (positions[1:, fid_r, 2] - positions[:-1, fid_r, 2]) ** 2\n",
    "        feet_r = ((feet_r_x + feet_r_y + feet_r_z) < velfactor).astype(np.float32)\n",
    "        return feet_l, feet_r\n",
    "\n",
    "    feet_l, feet_r = foot_detect(positions, feet_thre)\n",
    "\n",
    "    r_rot = None\n",
    "\n",
    "    def get_rifke(positions):\n",
    "        # root 좌표 기준 local로\n",
    "        positions[..., 0] -= positions[:, 0:1, 0]\n",
    "        positions[..., 2] -= positions[:, 0:1, 2]\n",
    "        # 모든 포즈를 Z+로 맞추기\n",
    "        positions_ = qrot_np(np.repeat(r_rot[:, None], positions.shape[1], axis=1), positions)\n",
    "        return positions_\n",
    "\n",
    "    def get_cont6d_params(positions):\n",
    "        skel = Skeleton(n_raw_offsets, kinematic_chain, \"cpu\")\n",
    "        quat_params = skel.inverse_kinematics_np(positions, face_joint_indx, smooth_forward=True)\n",
    "\n",
    "        cont_6d_params = quaternion_to_cont6d_np(quat_params)\n",
    "        r_rot_local = quat_params[:, 0].copy()\n",
    "\n",
    "        velocity = (positions[1:, 0] - positions[:-1, 0]).copy()\n",
    "        velocity = qrot_np(r_rot_local[1:], velocity)\n",
    "\n",
    "        r_velocity = qmul_np(r_rot_local[1:], qinv_np(r_rot_local[:-1]))\n",
    "        return cont_6d_params, r_velocity, velocity, r_rot_local\n",
    "\n",
    "    cont_6d_params, r_velocity, velocity, r_rot = get_cont6d_params(positions)\n",
    "    positions = get_rifke(positions)\n",
    "\n",
    "    # root height\n",
    "    root_y = positions[:, 0, 1:2]\n",
    "\n",
    "    # root rotation / linear velocity\n",
    "    r_velocity = np.arcsin(r_velocity[:, 2:3])\n",
    "    l_velocity = velocity[:, [0, 2]]\n",
    "    root_data = np.concatenate([r_velocity, l_velocity, root_y[:-1]], axis=-1)\n",
    "\n",
    "    # joint rotation (cont6d)\n",
    "    rot_data = cont_6d_params[:, 1:].reshape(len(cont_6d_params), -1)\n",
    "\n",
    "    # joint local position (ric)\n",
    "    ric_data = positions[:, 1:].reshape(len(positions), -1)\n",
    "\n",
    "    # local velocity\n",
    "    local_vel = qrot_np(\n",
    "        np.repeat(r_rot[:-1, None], global_positions.shape[1], axis=1),\n",
    "        global_positions[1:] - global_positions[:-1]\n",
    "    )\n",
    "    local_vel = local_vel.reshape(len(local_vel), -1)\n",
    "\n",
    "    data = root_data\n",
    "    data = np.concatenate([data, ric_data[:-1]], axis=-1)\n",
    "    data = np.concatenate([data, rot_data[:-1]], axis=-1)\n",
    "    data = np.concatenate([data, local_vel], axis=-1)\n",
    "    data = np.concatenate([data, feet_l, feet_r], axis=-1)\n",
    "\n",
    "    return data, global_positions, positions, l_velocity\n",
    "\n",
    "\n",
    "def recover_root_rot_pos(data):\n",
    "    rot_vel = data[..., 0]\n",
    "    r_rot_ang = torch.zeros_like(rot_vel).to(data.device)\n",
    "    r_rot_ang[..., 1:] = rot_vel[..., :-1]\n",
    "    r_rot_ang = torch.cumsum(r_rot_ang, dim=-1)\n",
    "\n",
    "    r_rot_quat = torch.zeros(data.shape[:-1] + (4,)).to(data.device)\n",
    "    r_rot_quat[..., 0] = torch.cos(r_rot_ang)\n",
    "    r_rot_quat[..., 2] = torch.sin(r_rot_ang)\n",
    "\n",
    "    r_pos = torch.zeros(data.shape[:-1] + (3,)).to(data.device)\n",
    "    r_pos[..., 1:, [0, 2]] = data[..., :-1, 1:3]\n",
    "    r_pos = qrot(qinv(r_rot_quat), r_pos)\n",
    "    r_pos = torch.cumsum(r_pos, dim=-2)\n",
    "    r_pos[..., 1] = data[..., 3]\n",
    "    return r_rot_quat, r_pos\n",
    "\n",
    "\n",
    "def recover_from_rot(data, joints_num, skeleton):\n",
    "    r_rot_quat, r_pos = recover_root_rot_pos(data)\n",
    "    r_rot_cont6d = quaternion_to_cont6d(r_rot_quat)\n",
    "\n",
    "    start_indx = 1 + 2 + 1 + (joints_num - 1) * 3\n",
    "    end_indx = start_indx + (joints_num - 1) * 6\n",
    "    cont6d_params = data[..., start_indx:end_indx]\n",
    "\n",
    "    cont6d_params = torch.cat([r_rot_cont6d, cont6d_params], dim=-1)\n",
    "    cont6d_params = cont6d_params.view(-1, joints_num, 6)\n",
    "    positions = skeleton.forward_kinematics_cont6d(cont6d_params, r_pos)\n",
    "    return positions\n",
    "\n",
    "\n",
    "def recover_from_ric(data, joints_num):\n",
    "    r_rot_quat, r_pos = recover_root_rot_pos(data)\n",
    "    positions = data[..., 4:(joints_num - 1) * 3 + 4]\n",
    "    positions = positions.view(positions.shape[:-1] + (-1, 3))\n",
    "\n",
    "    positions = qrot(\n",
    "        qinv(r_rot_quat[..., None, :]).expand(positions.shape[:-1] + (4,)),\n",
    "        positions\n",
    "    )\n",
    "\n",
    "    positions[..., 0] += r_pos[..., 0:1]\n",
    "    positions[..., 2] += r_pos[..., 2:3]\n",
    "\n",
    "    positions = torch.cat([r_pos.unsqueeze(-2), positions], dim=-2)\n",
    "    return positions\n",
    "\n",
    "\n",
    "# ------------------ main for HumanML3D ------------------ #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 예시로 사용할 클립 id (joints에 실제로 존재해야 함)\n",
    "    example_id = \"000021\"\n",
    "\n",
    "    # Lower legs indices\n",
    "    l_idx1, l_idx2 = 5, 8\n",
    "    # Right/Left foot joint indices\n",
    "    fid_r, fid_l = [8, 11], [7, 10]\n",
    "    # Face direction, r_hip, l_hip, sdr_r, sdr_l\n",
    "    face_joint_indx = [2, 1, 17, 16]\n",
    "    # hips\n",
    "    r_hip, l_hip = 2, 1\n",
    "    joints_num = 22\n",
    "\n",
    "    data_dir = './joints/'\n",
    "    save_dir1 = './HumanML3D/new_joints/'\n",
    "    save_dir2 = './HumanML3D/new_joint_vecs/'\n",
    "\n",
    "    os.makedirs(save_dir1, exist_ok=True)\n",
    "    os.makedirs(save_dir2, exist_ok=True)\n",
    "\n",
    "    # paramUtil 에서 가져온 raw offsets / kinematic chain\n",
    "    n_raw_offsets = torch.from_numpy(t2m_raw_offsets)\n",
    "    kinematic_chain = t2m_kinematic_chain\n",
    "\n",
    "    # 타겟 스켈레톤 offset 추출\n",
    "    example_path = os.path.join(data_dir, example_id + '.npy')\n",
    "    if not os.path.exists(example_path):\n",
    "        raise FileNotFoundError(f\"Example file not found: {example_path}\")\n",
    "\n",
    "    example_data = np.load(example_path)\n",
    "    example_data = example_data.reshape(len(example_data), -1, 3)\n",
    "    example_data = torch.from_numpy(example_data)\n",
    "    tgt_skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
    "    tgt_offsets = tgt_skel.get_offsets_joints(example_data[0])\n",
    "\n",
    "    # joints 폴더 안에서 .npy만 처리 (Windows 안전)\n",
    "    source_list = sorted(\n",
    "        [f for f in os.listdir(data_dir) if f.lower().endswith('.npy')]\n",
    "    )\n",
    "\n",
    "    frame_num = 0\n",
    "    for source_file in tqdm(source_list, desc=\"Processing joints\"):\n",
    "        src_path = os.path.join(data_dir, source_file)\n",
    "\n",
    "        try:\n",
    "            source_data = np.load(src_path)[:, :joints_num]\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] cannot load {src_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data, ground_positions, positions, l_velocity = process_file(source_data, 0.002)\n",
    "            rec_ric_data = recover_from_ric(\n",
    "                torch.from_numpy(data).unsqueeze(0).float(),\n",
    "                joints_num\n",
    "            )\n",
    "            np.save(pjoin(save_dir1, source_file), rec_ric_data.squeeze().numpy())\n",
    "            np.save(pjoin(save_dir2, source_file), data)\n",
    "            frame_num += data.shape[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {source_file}: {e}\")\n",
    "\n",
    "    print('Total clips: %d, Frames: %d, Duration: %fm' %\n",
    "          (len(source_list), frame_num, frame_num / 20 / 60))\n",
    "\n",
    "    # 옵션: reference 파일 체크 (있을 때만)\n",
    "    ref_joints_path = './HumanML3D/new_joints/012314.npy'\n",
    "    ref_vecs_path = './HumanML3D/new_joint_vecs/012314.npy'\n",
    "    if os.path.exists(ref_joints_path) and os.path.exists(ref_vecs_path):\n",
    "        reference1 = np.load(ref_joints_path)\n",
    "        reference2 = np.load(ref_vecs_path)\n",
    "        print(\"Loaded reference clips 012314 for sanity check.\")\n",
    "    else:\n",
    "        print(\"Reference files 012314 not found yet (this is normal on first run).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if your data is correct. If it's aligned with the given reference, then it is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference1_1 = np.load('./HumanML3D/new_joints/012314.npy')\n",
    "reference2_1 = np.load('./HumanML3D/new_joint_vecs/012314.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(reference1 - reference1_1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(reference2 - reference2_1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
