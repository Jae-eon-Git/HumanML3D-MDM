{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a037116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\21600\\Downloads\\assignment\\HumanML3D\\HumanML3D\n",
      "HumanML3D folder: C:\\Users\\21600\\Downloads\\assignment\\HumanML3D\\HumanML3D\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "\n",
    "print(\"Project root:\", ROOT)\n",
    "print(\"HumanML3D folder:\", ROOT)\n",
    "\n",
    "MAX_SEQ_LEN = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7485e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bcf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATASET & COLLATE\n",
    "\n",
    "class HumanML3DDataset(Dataset):\n",
    "    \"\"\"\n",
    "    HumanML3D 구조:\n",
    "        Mean.npy, Std.npy\n",
    "        train.txt / val.txt / test.txt\n",
    "        new_joint_vecs/<id>.npy (motion: T, 263)\n",
    "        texts/<id>.txt          (caption lines)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, split=\"train\", normalize=True):\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.normalize = normalize\n",
    "\n",
    "        split_file = os.path.join(data_root, f\"{split}.txt\")\n",
    "        with open(split_file, \"r\") as f:\n",
    "            ids = [line.strip().replace(\".npy\", \"\") for line in f.readlines()]\n",
    "        self.sample_ids = ids\n",
    "\n",
    "        self.motion_dir = os.path.join(data_root, \"new_joint_vecs\")\n",
    "        self.text_dir = os.path.join(data_root, \"texts\")\n",
    "\n",
    "        # 정규화 (Mean, Std)\n",
    "        if normalize:\n",
    "            self.mean = np.load(os.path.join(data_root, \"Mean.npy\"))  # (263,)\n",
    "            self.std = np.load(os.path.join(data_root, \"Std.npy\"))    # (263,)\n",
    "        else:\n",
    "            self.mean = None\n",
    "            self.std = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def _load_motion(self, sid):\n",
    "        path = os.path.join(self.motion_dir, sid + \".npy\")\n",
    "        motion = np.load(path).astype(np.float32)  # (T, 263)\n",
    "\n",
    "        if self.normalize:\n",
    "            motion = (motion - self.mean) / (self.std + 1e-8)\n",
    "\n",
    "        return motion\n",
    "\n",
    "    def _load_text(self, sid):\n",
    "        path = os.path.join(self.text_dir, sid + \".txt\")\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "        raw = random.choice(lines)\n",
    "        caption = raw.split(\"#\")[0].lower()\n",
    "        return caption\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.sample_ids[idx]\n",
    "        motion = self._load_motion(sid)\n",
    "        caption = self._load_text(sid)\n",
    "\n",
    "        return {\n",
    "            \"name\": sid,\n",
    "            \"motion\": motion,   # numpy (T, 263)\n",
    "            \"length\": motion.shape[0],\n",
    "            \"text\": caption\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307ac46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def humanml_collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: x[\"length\"], reverse=True)\n",
    "\n",
    "    lengths = [b[\"length\"] for b in batch]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    motions = []\n",
    "    texts = []\n",
    "    names = []\n",
    "\n",
    "    for b in batch:\n",
    "        m = b[\"motion\"]   # (T, 263)\n",
    "        T, D = m.shape\n",
    "\n",
    "        # padding to max_len\n",
    "        if T < max_len:\n",
    "            pad = np.zeros((max_len - T, D), dtype=np.float32)\n",
    "            m = np.concatenate([m, pad], axis=0)\n",
    "\n",
    "        motions.append(torch.from_numpy(m))   # (max_len, 263)\n",
    "        texts.append(b[\"text\"])\n",
    "        names.append(b[\"name\"])\n",
    "\n",
    "    motions = torch.stack(motions)   # (B, max_len, 263)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"motion\": motions,\n",
    "        \"length\": lengths,\n",
    "        \"text\": texts,\n",
    "        \"name\": names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e334138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21600\\miniconda3\\envs\\mdm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f736bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", pooling=\"cls\"):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "\n",
    "        # Load tokenizer & model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        # 출력 차원: 768\n",
    "        self.embed_dim = self.bert.config.hidden_size\n",
    "\n",
    "    def forward(self, text_list):\n",
    "\n",
    "        # Tokenize\n",
    "        enc = self.tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(next(self.bert.parameters()).device)\n",
    "\n",
    "        out = self.bert(\n",
    "            input_ids=enc[\"input_ids\"],\n",
    "            attention_mask=enc[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            text_emb = out.last_hidden_state[:, 0]  # (B, 768)\n",
    "        else:\n",
    "            mask = enc[\"attention_mask\"].unsqueeze(-1).float()  # (B, T, 1)\n",
    "            sum_hidden = (out.last_hidden_state * mask).sum(dim=1)\n",
    "            lengths = mask.sum(dim=1)\n",
    "            text_emb = sum_hidden / lengths\n",
    "\n",
    "        return text_emb  # (B, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d65a87-b00d-4ea8-a5b7-b2881c4e8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6284c677-eb43-4da9-934c-2680ed100996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MotionDiffusionTransformerConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        motion_dim=263,       # HumanML3D motion vector dimension\n",
    "        text_dim=768,         # BERT embedding dim\n",
    "        proj_dim=256,         # we project both into 256-d\n",
    "        model_dim=512,        # Transformer hidden dimension\n",
    "        num_layers=8,\n",
    "        num_heads=8,\n",
    "        ff_dim=1024,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # motion → 256\n",
    "        self.motion_proj = nn.Linear(motion_dim, proj_dim)\n",
    "\n",
    "        # text → 256\n",
    "        self.text_proj = nn.Linear(text_dim, proj_dim)\n",
    "\n",
    "        # concat → 512\n",
    "        self.input_dim = proj_dim * 2\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(model_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.match_dim = nn.Linear(self.input_dim, model_dim)\n",
    "\n",
    "        self.output_proj = nn.Linear(model_dim, motion_dim)\n",
    "\n",
    "    def forward(self, x, text_emb):\n",
    "        \"\"\"\n",
    "        x: (B, T, motion_dim)\n",
    "        text_emb: (B, text_dim)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # project motion → (B, T, 256)\n",
    "        motion_feat = self.motion_proj(x)\n",
    "\n",
    "        # project text → (B, 256)\n",
    "        text_feat = self.text_proj(text_emb)\n",
    "        text_feat = text_feat[:, None, :].repeat(1, T, 1)  # (B, T, 256)\n",
    "\n",
    "        # concat → (B, T, 512)\n",
    "        h = torch.cat([motion_feat, text_feat], dim=-1)\n",
    "\n",
    "        h = self.match_dim(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        out = self.output_proj(h)  # (B, T, 263)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c42edee-c7f4-46aa-adfb-082f9827a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(num_steps=1000, start=1e-4, end=0.02):\n",
    "    return torch.linspace(start, end, num_steps)\n",
    "\n",
    "\n",
    "class DiffusionSchedule:\n",
    "    def __init__(self, num_steps=1000, device=\"cpu\"):\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "        \n",
    "        self.betas = make_beta_schedule(num_steps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "    def extract(self, arr, t, x_shape):\n",
    "        \"\"\"\n",
    "        arr: (num_steps,)\n",
    "        t: (B,)\n",
    "        return shape: (B, 1, 1)\n",
    "        \"\"\"\n",
    "        return arr[t].view(-1, *([1] * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def q_sample(x0, t, schedule: DiffusionSchedule, noise=None):\n",
    "    \"\"\"\n",
    "    x0: (B, T, D)\n",
    "    t: (B,)\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "\n",
    "    sqrt_alpha_cumprod_t = schedule.extract(schedule.sqrt_alpha_cumprod, t, x0.shape)\n",
    "    sqrt_one_minus_alpha_cumprod_t = schedule.extract(schedule.sqrt_one_minus_alpha_cumprod, t, x0.shape)\n",
    "\n",
    "    x_t = sqrt_alpha_cumprod_t * x0 + sqrt_one_minus_alpha_cumprod_t * noise\n",
    "    return x_t, noise\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Loss\n",
    "# =========================\n",
    "\n",
    "def diffusion_loss(model, schedule, x0, text_emb):\n",
    "    B = x0.shape[0]\n",
    "    device = x0.device\n",
    "\n",
    "    t = torch.randint(0, schedule.num_steps, (B,), device=device)\n",
    "    x_t, noise = q_sample(x0, t, schedule)\n",
    "\n",
    "    eps_pred = model(x_t, text_emb)\n",
    "\n",
    "    return F.mse_loss(eps_pred, noise)\n",
    "\n",
    "def p_sample(model, schedule, x_t, t, text_emb, eta=0.0):\n",
    " \n",
    "    eps_pred = model(x_t, text_emb)   # (B, T, D)\n",
    "    alphas = schedule.alphas          # (num_steps,)\n",
    "    alpha_t = alphas[t].view(-1, 1, 1)       # (B,1,1)\n",
    "\n",
    "    sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "    sqrt_one_minus_alpha_t = torch.sqrt(1.0 - alpha_t)\n",
    "\n",
    "    x_prev = (x_t - sqrt_one_minus_alpha_t * eps_pred) / sqrt_alpha_t\n",
    "\n",
    "    return x_prev\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, schedule, shape, text_emb, device=\"cpu\", eta=0.0):\n",
    "    B = shape[0]\n",
    "    x_t = torch.randn(shape, device=device)\n",
    "    text_emb = text_emb.to(device)\n",
    "\n",
    "    for step in reversed(range(schedule.num_steps)):\n",
    "        t = torch.full((B,), step, device=device, dtype=torch.long)\n",
    "        x_t = p_sample(model, schedule, x_t, t, text_emb, eta=eta)\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "355c38f1-7413-4902-991f-eab8d0ea9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(\n",
    "    data_root,\n",
    "    save_dir=\"./checkpoints\",\n",
    "    num_epochs=50,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    num_diffusion_steps=1000,\n",
    "    device=None,\n",
    "    finetune_bert=False,\n",
    "    num_workers=0,         \n",
    "    log_interval=50\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_dataset = HumanML3DDataset(\n",
    "        data_root=data_root,\n",
    "        split=\"train\",\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=humanml_collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    text_encoder = BERTTextEncoder(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        pooling=\"cls\"\n",
    "    ).to(device)\n",
    "\n",
    "    motion_model = MotionDiffusionTransformerConcat(\n",
    "        motion_dim=263,\n",
    "        text_dim=text_encoder.embed_dim,\n",
    "        proj_dim=256,\n",
    "        model_dim=512,\n",
    "        num_layers=8,\n",
    "        num_heads=8,\n",
    "        ff_dim=1024,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    if not finetune_bert:\n",
    "        for p in text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        text_encoder.eval()\n",
    "        print(\"[INFO] BERT encoder is frozen (no finetuning).\")\n",
    "    else:\n",
    "        print(\"[INFO] BERT encoder will be finetuned.\")\n",
    "\n",
    "    schedule = DiffusionSchedule(num_steps=num_diffusion_steps, device=device)\n",
    "\n",
    "    if finetune_bert:\n",
    "        params = list(text_encoder.parameters()) + list(motion_model.parameters())\n",
    "    else:\n",
    "        params = list(motion_model.parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr)\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        motion_model.train()\n",
    "        if finetune_bert:\n",
    "            text_encoder.train()\n",
    "        else:\n",
    "            text_encoder.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(\n",
    "            enumerate(train_loader),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch {epoch}/{num_epochs}\"\n",
    "        )\n",
    "\n",
    "        for batch_idx, batch in pbar:\n",
    "            motions = batch[\"motion\"].to(device)  # (B, T, 263)\n",
    "            texts = batch[\"text\"]                 # list[str]\n",
    "\n",
    "            if finetune_bert:\n",
    "                text_emb = text_encoder(texts)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    text_emb = text_encoder(texts)\n",
    "\n",
    "            loss = diffusion_loss(\n",
    "                model=motion_model,\n",
    "                schedule=schedule,\n",
    "                x0=motions,\n",
    "                text_emb=text_emb\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_val = loss.item()\n",
    "            running_loss += loss_val\n",
    "            global_step += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"step_loss\": f\"{loss_val:.6f}\",\n",
    "                \"avg_loss\": f\"{running_loss / (batch_idx + 1):.6f}\"\n",
    "            })\n",
    "\n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                avg_loss = running_loss / (batch_idx + 1)\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}] \"\n",
    "                    f\"Step {batch_idx+1:04d}/{len(train_loader):04d} | \"\n",
    "                    f\"Global {global_step:06d} | \"\n",
    "                    f\"step_loss: {loss_val:.6f} | \"\n",
    "                    f\"avg_loss: {avg_loss:.6f}\"\n",
    "                )\n",
    "\n",
    "        ckpt_path = os.path.join(save_dir, f\"epoch_{epoch:03d}.pt\")\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"motion_model_state\": motion_model.state_dict(),\n",
    "            \"text_encoder_state\": text_encoder.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"config\": {\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"lr\": lr,\n",
    "                \"num_diffusion_steps\": num_diffusion_steps,\n",
    "                \"finetune_bert\": finetune_bert,\n",
    "            }\n",
    "        }\n",
    "        torch.save(save_dict, ckpt_path)\n",
    "        print(f\"[INFO] Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "980283dc-be14-4df0-9ab2-20692784c50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] BERT encoder is frozen (no finetuning).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|                           | 1/1461 [00:02<1:05:50,  2.71s/it, step_loss=1.338799, avg_loss=1.338799]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# HumanML3D가 ROOT 바로 아래에 있다고 가정\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     data_root \u001b[38;5;241m=\u001b[39m ROOT\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints_humanml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_diffusion_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinetune_bert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# 처음엔 False로 두고 실험 추천\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Windows면 0\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 112\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_root, save_dir, num_epochs, batch_size, lr, num_diffusion_steps, device, finetune_bert, num_workers, log_interval)\u001b[0m\n\u001b[0;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m diffusion_loss(\n\u001b[0;32m    105\u001b[0m     model\u001b[38;5;241m=\u001b[39mmotion_model,\n\u001b[0;32m    106\u001b[0m     schedule\u001b[38;5;241m=\u001b[39mschedule,\n\u001b[0;32m    107\u001b[0m     x0\u001b[38;5;241m=\u001b[39mmotions,\n\u001b[0;32m    108\u001b[0m     text_emb\u001b[38;5;241m=\u001b[39mtext_emb\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 112\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(params, max_grad_norm)\n\u001b[0;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\mdm\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\mdm\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\mdm\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_root = ROOT\n",
    "\n",
    "    train(\n",
    "        data_root=data_root,\n",
    "        save_dir=os.path.join(ROOT, \"checkpoints_humanml2\"),\n",
    "        num_epochs=100,\n",
    "        batch_size=16,\n",
    "        lr=1e-3,\n",
    "        num_diffusion_steps=20,\n",
    "        finetune_bert=False,      \n",
    "        num_workers=0,            # Windows\n",
    "        log_interval=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc4261-d239-4ebc-98ab-306723987931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a4ff352-d949-4203-920b-c3bc3fc4a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skeleton import Skeleton               \n",
    "from paramUtil import t2m_raw_offsets, t2m_kinematic_chain\n",
    "from common.quaternion import qrot, qinv       \n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "JOINT_NUM = 22\n",
    "FACE_JOINT_IDX = [2, 1, 17, 16]\n",
    "\n",
    "JOINT_NAMES = [\n",
    "    \"Hips\",          # 0\n",
    "    \"LeftUpLeg\",     # 1\n",
    "    \"RightUpLeg\",    # 2\n",
    "    \"Spine\",         # 3\n",
    "    \"LeftLeg\",       # 4\n",
    "    \"RightLeg\",      # 5\n",
    "    \"Spine1\",        # 6\n",
    "    \"LeftFoot\",      # 7\n",
    "    \"RightFoot\",     # 8\n",
    "    \"Spine2\",        # 9\n",
    "    \"LeftToeBase\",   # 10\n",
    "    \"RightToeBase\",  # 11\n",
    "    \"Neck\",          # 12\n",
    "    \"RightShoulder\", # 13\n",
    "    \"LeftShoulder\",  # 14\n",
    "    \"RightArm\",      # 15\n",
    "    \"LeftArm\",       # 16\n",
    "    \"RightForeArm\",  # 17\n",
    "    \"LeftForeArm\",   # 18\n",
    "    \"RightHand\",     # 19\n",
    "    \"LeftHand\",      # 20\n",
    "    \"LeftHandEnd\"    # 21 \n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  1. 263 → joint 위치 복원\n",
    "# =========================\n",
    "\n",
    "def recover_root_rot_pos(data: torch.Tensor):\n",
    "    \"\"\"\n",
    "    data: (..., T, D=263) torch tensor\n",
    "    반환:\n",
    "        r_rot_quat: (..., T, 4)\n",
    "        r_pos:      (..., T, 3)\n",
    "    \"\"\"\n",
    "    rot_vel = data[..., 0]\n",
    "    r_rot_ang = torch.zeros_like(rot_vel).to(data.device)\n",
    "    r_rot_ang[..., 1:] = rot_vel[..., :-1]\n",
    "    r_rot_ang = torch.cumsum(r_rot_ang, dim=-1)\n",
    "\n",
    "    r_rot_quat = torch.zeros(data.shape[:-1] + (4,), device=data.device)\n",
    "    r_rot_quat[..., 0] = torch.cos(r_rot_ang)\n",
    "    r_rot_quat[..., 2] = torch.sin(r_rot_ang)\n",
    "\n",
    "    r_pos = torch.zeros(data.shape[:-1] + (3,), device=data.device)\n",
    "    # x, z 속도 적분\n",
    "    r_pos[..., 1:, [0, 2]] = data[..., :-1, 1:3]\n",
    "    r_pos = qrot(qinv(r_rot_quat), r_pos)\n",
    "    r_pos = torch.cumsum(r_pos, dim=-2)\n",
    "    # y 높이\n",
    "    r_pos[..., 1] = data[..., 3]\n",
    "    return r_rot_quat, r_pos\n",
    "\n",
    "\n",
    "def recover_from_ric(data: torch.Tensor, joints_num: int):\n",
    "    \"\"\"\n",
    "    HumanML3D 263D (RIC 기반 representation)에서 joint 위치로 복원.\n",
    "    data: (B, T, D=263)\n",
    "    반환:\n",
    "        positions: (B, T, joints_num, 3)\n",
    "    \"\"\"\n",
    "    r_rot_quat, r_pos = recover_root_rot_pos(data)\n",
    "\n",
    "    # RIC part: joint local positions (root 제거된 것) \n",
    "    positions = data[..., 4:(joints_num - 1) * 3 + 4]\n",
    "    positions = positions.view(positions.shape[:-1] + (-1, 3))  # (..., T, J-1, 3)\n",
    "\n",
    "    # root 회전 적용해서 월드 좌표로\n",
    "    positions = qrot(\n",
    "        qinv(r_rot_quat[..., None, :]).expand(positions.shape[:-1] + (4,)),\n",
    "        positions\n",
    "    )\n",
    "\n",
    "    # root 위치 더해주기\n",
    "    positions[..., 0] += r_pos[..., 0:1]\n",
    "    positions[..., 2] += r_pos[..., 2:3]\n",
    "\n",
    "    # root joint 붙이기\n",
    "    positions = torch.cat([r_pos.unsqueeze(-2), positions], dim=-2)\n",
    "    return positions\n",
    "\n",
    "\n",
    "def humanml3d_263_to_positions(motion_263: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    motion_263: (T, 263) numpy\n",
    "    반환: positions (T, 22, 3) numpy\n",
    "    \"\"\"\n",
    "    assert motion_263.ndim == 2 and motion_263.shape[1] == 263\n",
    "    data = torch.from_numpy(motion_263).float().unsqueeze(0)  # (1, T, 263)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        positions = recover_from_ric(data, JOINT_NUM)[0]      # (T, 22, 3)\n",
    "\n",
    "    return positions.cpu().numpy()\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  2. joint 위치 → joint 회전 (quaternion)  → Euler\n",
    "# =========================\n",
    "\n",
    "def build_skeleton(device=\"cpu\"):\n",
    "    offsets = torch.from_numpy(t2m_raw_offsets).float()\n",
    "    skel = Skeleton(offsets, t2m_kinematic_chain, device)     # :contentReference[oaicite:3]{index=3}\n",
    "    return skel\n",
    "\n",
    "\n",
    "def positions_to_quat(positions: np.ndarray, skel: Skeleton) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    positions: (T, J, 3) numpy\n",
    "    반환: quat_params (T, J, 4) numpy, [w, x, y, z]\n",
    "    \"\"\"\n",
    "    assert positions.ndim == 3\n",
    "    quat_params = skel.inverse_kinematics_np(\n",
    "        positions, FACE_JOINT_IDX, smooth_forward=True\n",
    "    )  # (T, J, 4)\n",
    "    return quat_params\n",
    "\n",
    "\n",
    "def quat_to_euler_zyx(q: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    q: (..., 4) numpy, [w, x, y, z]\n",
    "    반환: (..., 3) Euler angles [Z, X, Y] in degrees (BVH 채널 순서용)\n",
    "    SciPy는 [x, y, z, w] 를 쓰므로 순서 변환 필요.\n",
    "    \"\"\"\n",
    "    # reshape to (-1, 4)\n",
    "    orig_shape = q.shape[:-1]\n",
    "    q_flat = q.reshape(-1, 4)\n",
    "\n",
    "    # [w, x, y, z] -> [x, y, z, w]\n",
    "    q_scipy = np.stack(\n",
    "        [q_flat[:, 1], q_flat[:, 2], q_flat[:, 3], q_flat[:, 0]],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    rot = R.from_quat(q_scipy)\n",
    "    euler = rot.as_euler('ZXY', degrees=True)  # (N, 3)\n",
    "    return euler.reshape(orig_shape + (3,))\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  3. BVH 파일 쓰기\n",
    "# =========================\n",
    "\n",
    "def build_parents_and_children(skel: Skeleton):\n",
    "    parents = skel.parents()               # len J, root 0 의 parent == -1 :contentReference[oaicite:4]{index=4}\n",
    "    J = len(parents)\n",
    "    children = [[] for _ in range(J)]\n",
    "    for j in range(J):\n",
    "        p = parents[j]\n",
    "        if p >= 0:\n",
    "            children[p].append(j)\n",
    "    return parents, children\n",
    "\n",
    "\n",
    "def write_joint_recursive(f, joint_idx, parents, children, offsets, level=0):\n",
    "    \"\"\"\n",
    "    재귀적으로 HIERARCHY 블럭 작성.\n",
    "    offsets: (J, 3)\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    name = JOINT_NAMES[joint_idx] if joint_idx < len(JOINT_NAMES) else f\"J{joint_idx}\"\n",
    "\n",
    "    if parents[joint_idx] == -1:\n",
    "        # ROOT joint\n",
    "        f.write(f\"{indent}ROOT {name}\\n\")\n",
    "    else:\n",
    "        f.write(f\"{indent}JOINT {name}\\n\")\n",
    "\n",
    "    f.write(f\"{indent}{{\\n\")\n",
    "    off = offsets[joint_idx]\n",
    "    f.write(f\"{indent}  OFFSET {off[0]:.6f} {off[1]:.6f} {off[2]:.6f}\\n\")\n",
    "\n",
    "    if parents[joint_idx] == -1:\n",
    "        # ROOT: position + rotation\n",
    "        f.write(f\"{indent}  CHANNELS 6 Xposition Yposition Zposition Zrotation Xrotation Yrotation\\n\")\n",
    "    else:\n",
    "        f.write(f\"{indent}  CHANNELS 3 Zrotation Xrotation Yrotation\\n\")\n",
    "\n",
    "    # children joints\n",
    "    for c in children[joint_idx]:\n",
    "        write_joint_recursive(f, c, parents, children, offsets, level + 1)\n",
    "\n",
    "    # End Site (leaf joint일 때만 적당히 하나 만들어 줌)\n",
    "    if len(children[joint_idx]) == 0:\n",
    "        f.write(f\"{indent}  End Site\\n\")\n",
    "        f.write(f\"{indent}  {{\\n\")\n",
    "        # 그냥 대략적인 길이로 0.1 넣음\n",
    "        f.write(f\"{indent}    OFFSET 0.000000 0.000000 0.100000\\n\")\n",
    "        f.write(f\"{indent}  }}\\n\")\n",
    "\n",
    "    f.write(f\"{indent}}}\\n\")\n",
    "\n",
    "def write_bvh_fixed_offsets(path, positions, eulers, offsets, parents, frame_time):\n",
    "\n",
    "    T, J, _ = positions.shape\n",
    "\n",
    "    # ----- children list 만들기 -----\n",
    "    children = [[] for _ in range(J)]\n",
    "    for j in range(J):\n",
    "        p = parents[j]\n",
    "        if p >= 0:\n",
    "            children[p].append(j)\n",
    "\n",
    "    # ----- joint names -----\n",
    "    names = JOINT_NAMES\n",
    "\n",
    "    # ----- 재귀 출력 함수 -----\n",
    "    def write_joint(f, j, level=0):\n",
    "        indent = \"  \" * level\n",
    "        name = names[j]\n",
    "\n",
    "        if parents[j] == -1:\n",
    "            f.write(f\"{indent}ROOT {name}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{indent}JOINT {name}\\n\")\n",
    "\n",
    "        f.write(f\"{indent}{{\\n\")\n",
    "\n",
    "        off = offsets[j]\n",
    "        f.write(f\"{indent}  OFFSET {off[0]:.6f} {off[1]:.6f} {off[2]:.6f}\\n\")\n",
    "\n",
    "        if parents[j] == -1:\n",
    "            f.write(f\"{indent}  CHANNELS 6 Xposition Yposition Zposition Zrotation Xrotation Yrotation\\n\")\n",
    "        else:\n",
    "            f.write(f\"{indent}  CHANNELS 3 Zrotation Xrotation Yrotation\\n\")\n",
    "\n",
    "        for c in children[j]:\n",
    "            write_joint(f, c, level + 1)\n",
    "\n",
    "        if len(children[j]) == 0:\n",
    "            f.write(f\"{indent}  End Site\\n\")\n",
    "            f.write(f\"{indent}  {{\\n\")\n",
    "            f.write(f\"{indent}    OFFSET 0 0 0.1\\n\")\n",
    "            f.write(f\"{indent}  }}\\n\")\n",
    "\n",
    "        f.write(f\"{indent}}}\\n\")\n",
    "\n",
    "    # ----- 파일 작성 시작 -----\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"HIERARCHY\\n\")\n",
    "        write_joint(f, 0, 0)\n",
    "\n",
    "        f.write(\"MOTION\\n\")\n",
    "        f.write(f\"Frames: {T}\\n\")\n",
    "        f.write(f\"Frame Time: {frame_time:.8f}\\n\")\n",
    "\n",
    "        # preorder\n",
    "        order = []\n",
    "        def collect(j):\n",
    "            order.append(j)\n",
    "            for c in children[j]:\n",
    "                collect(c)\n",
    "        collect(0)\n",
    "\n",
    "        for t in range(T):\n",
    "            vals = []\n",
    "            for j in order:\n",
    "                pos = positions[t, j]\n",
    "                rot = eulers[t, j]  # Z X Y\n",
    "                if parents[j] == -1:\n",
    "                    vals.extend([pos[0], pos[1], pos[2], rot[0], rot[1], rot[2]])\n",
    "                else:\n",
    "                    vals.extend([rot[0], rot[1], rot[2]])\n",
    "\n",
    "            f.write(\" \".join(f\"{v:.6f}\" for v in vals) + \"\\n\")\n",
    "# =========================\n",
    "#  4. 전체 파이프라인 + CLI\n",
    "# =========================\n",
    "def convert_263_array_to_bvh(\n",
    "    motion_263: np.ndarray,\n",
    "    output_bvh: str,\n",
    "    frame_time: float = 1/20.0\n",
    "):\n",
    "    \"\"\"\n",
    "    motion_263: (T,263) numpy (denormalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- 1) 263D → joint positions --------\n",
    "    positions = humanml3d_263_to_positions(motion_263)   # (T,22,3)\n",
    "    # -------s- 2) Skeleton 준비 --------\n",
    "    skel = build_skeleton(device=\"cpu\")\n",
    "    parents = skel.parents()\n",
    "    J = len(parents)\n",
    "\n",
    "    # -------- 3) BVH 오프셋을 직접 계산 --------\n",
    "    first = positions[0]   # (22,3)\n",
    "    offsets = np.zeros_like(first)\n",
    "\n",
    "    for j in range(J):\n",
    "        p = parents[j]\n",
    "        if p == -1:\n",
    "            offsets[j] = np.array([0, 0, 0])   # root offset 항상 0\n",
    "        else:\n",
    "            offsets[j] = first[j] - first[p]   # local offset by position diff\n",
    "\n",
    "    # -------- 4) IK → quaternion --------\n",
    "    quat = positions_to_quat(positions, skel)     # (T,22,4)\n",
    "\n",
    "    # -------- 5) quaternion → Euler(Z,X,Y) --------\n",
    "    eulers = quat_to_euler_zyx(quat)\n",
    "\n",
    "    # -------- 6) BVH 저장 --------\n",
    "    write_bvh_fixed_offsets(\n",
    "        output_bvh,\n",
    "        positions,\n",
    "        eulers,\n",
    "        offsets,\n",
    "        parents,\n",
    "        frame_time\n",
    "    )\n",
    "    print(\"[OK] Saved BVH:\", output_bvh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0c8a509-ea69-4233-8dc9-13f6a87299dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(data_root, batch_size=4):\n",
    "    test_dataset = HumanML3DDataset(\n",
    "        data_root=data_root,\n",
    "        split=\"test\",\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=humanml_collate_fn,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "def load_inference_model(ckpt_path, device=\"cpu\"):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    text_encoder = BERTTextEncoder(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        pooling=\"cls\"\n",
    "    ).to(device)\n",
    "\n",
    "    motion_model = MotionDiffusionTransformerConcat(\n",
    "        motion_dim=263,\n",
    "        text_dim=text_encoder.embed_dim,\n",
    "        proj_dim=256,\n",
    "        model_dim=512,\n",
    "        num_layers=8,\n",
    "        num_heads=8,\n",
    "        ff_dim=1024,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    motion_model.load_state_dict(ckpt[\"motion_model_state\"])\n",
    "    text_encoder.load_state_dict(ckpt[\"text_encoder_state\"])\n",
    "\n",
    "    for p in text_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    text_encoder.eval()\n",
    "    motion_model.eval()\n",
    "\n",
    "    schedule = DiffusionSchedule(num_steps=120, device=device)\n",
    "\n",
    "    print(\"Loaded checkpoint:\", ckpt_path)\n",
    "    return text_encoder, motion_model, schedule\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_motion_from_text_batch(\n",
    "    text_encoder,\n",
    "    motion_model,\n",
    "    schedule,\n",
    "    texts,\n",
    "    T,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    text_emb = text_encoder(texts).to(device)\n",
    "\n",
    "    B = len(texts)\n",
    "    D = 263\n",
    "    shape = (B, T, D)\n",
    "\n",
    "    x0 = p_sample_loop(\n",
    "        model=motion_model,\n",
    "        schedule=schedule,\n",
    "        shape=shape,\n",
    "        text_emb=text_emb,\n",
    "        device=device\n",
    "    )\n",
    "    return x0  # normalized\n",
    "def generate_5_bvh(\n",
    "    data_root,\n",
    "    ckpt_path,\n",
    "    out_dir=\"bvh_samples\",\n",
    "    device=\"cpu\",\n",
    "    fps=20.0,\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    loader = get_test_loader(data_root, batch_size=4)\n",
    "    text_encoder, motion_model, schedule = load_inference_model(ckpt_path, device)\n",
    "\n",
    "    mean = np.load(os.path.join(data_root, \"Mean.npy\"))   # (263,)\n",
    "    std  = np.load(os.path.join(data_root, \"Std.npy\"))    # (263,)\n",
    "    frame_time = 1.0 / fps\n",
    "    saved = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        if saved >= 5:\n",
    "            break\n",
    "\n",
    "        names = batch[\"name\"]        \n",
    "        motions = batch[\"motion\"]    \n",
    "        texts   = batch[\"text\"]      \n",
    "        T = motions.shape[1]\n",
    "\n",
    "        x0_norm = generate_motion_from_text_batch(\n",
    "            text_encoder,\n",
    "            motion_model,\n",
    "            schedule,\n",
    "            texts,\n",
    "            T,\n",
    "            device\n",
    "        )   # (B,T,263), normalized\n",
    "\n",
    "        \n",
    "        for i in range(x0_norm.shape[0]):\n",
    "            if saved >= 5:\n",
    "                break\n",
    "\n",
    "            \n",
    "            motion_263_norm = x0_norm[i].cpu().numpy()  # (T,263)\n",
    "\n",
    "            # (2) denormalize: (x * std + mean)\n",
    "            motion_263 = motion_263_norm * std + mean   # (T,263) denorm\n",
    "\n",
    "            def show_stats(x, name):\n",
    "                print(f\"=== {name} ===\")\n",
    "                print(\"shape:\", x.shape)\n",
    "                print(\"min/max:\", x.min(), x.max())\n",
    "                print(\"mean/std:\", x.mean(), x.std())\n",
    "    \n",
    "            show_stats(motion_263, \"Gen\")\n",
    "\n",
    "            bvh_path = os.path.join(out_dir, f\"{names[i]}_gen.bvh\")\n",
    "\n",
    "            convert_263_array_to_bvh(\n",
    "                motion_263=motion_263,\n",
    "                output_bvh=bvh_path,\n",
    "                frame_time=frame_time,\n",
    "            )\n",
    "\n",
    "            print(f\"[{saved}] Saved BVH:\", bvh_path)\n",
    "            print(f\"    text:\", texts[i])\n",
    "            saved += 1\n",
    "\n",
    "    print(\"Done, saved 5 BVH files.\")\n",
    "    return motion_263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b56ed7a-f98e-4db9-861f-f736f029fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: .\\checkpoints_humanml\\epoch_034.pt\n",
      "=== Gen ===\n",
      "shape: (199, 263)\n",
      "min/max: -0.8211727 1.4934978\n",
      "mean/std: 0.19189551 0.41662708\n",
      "[OK] Saved BVH: .\\bvh_samples\\004822_gen.bvh\n",
      "[0] Saved BVH: .\\bvh_samples\\004822_gen.bvh\n",
      "    text: person walking at a average pace forward, swaying arms and torso with a sense of swagger\n",
      "=== Gen ===\n",
      "shape: (199, 263)\n",
      "min/max: -0.9096818 1.4815434\n",
      "mean/std: 0.18952002 0.41479903\n",
      "[OK] Saved BVH: .\\bvh_samples\\008463_gen.bvh\n",
      "[1] Saved BVH: .\\bvh_samples\\008463_gen.bvh\n",
      "    text: man walks along, then bends down and picks something up.\n",
      "=== Gen ===\n",
      "shape: (199, 263)\n",
      "min/max: -0.8293877 1.4810334\n",
      "mean/std: 0.18874314 0.4140141\n",
      "[OK] Saved BVH: .\\bvh_samples\\009613_gen.bvh\n",
      "[2] Saved BVH: .\\bvh_samples\\009613_gen.bvh\n",
      "    text: the person is running backwards quickly.\n",
      "=== Gen ===\n",
      "shape: (199, 263)\n",
      "min/max: -0.8484808 1.4817318\n",
      "mean/std: 0.191075 0.414631\n",
      "[OK] Saved BVH: .\\bvh_samples\\014457_gen.bvh\n",
      "[3] Saved BVH: .\\bvh_samples\\014457_gen.bvh\n",
      "    text: a person stands, bent slightly forward, holds onto something with both hands and swings their arms as if they were hitting a golf ball.\n",
      "=== Gen ===\n",
      "shape: (199, 263)\n",
      "min/max: -0.9426675 1.4815259\n",
      "mean/std: 0.18739359 0.4138446\n",
      "[OK] Saved BVH: .\\bvh_samples\\001969_gen.bvh\n",
      "[4] Saved BVH: .\\bvh_samples\\001969_gen.bvh\n",
      "    text: a person casually walks forwards, turns around, walks back, turns to the left, and backs up a few steps while raising both forearms level with the floor.\n",
      "Done, saved 5 BVH files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_root = \".\"   \n",
    "\n",
    "ckpt_path = os.path.join(data_root, \"checkpoints_humanml\", \"epoch_034.pt\")\n",
    "\n",
    "\n",
    "out_dir = os.path.join(data_root, \"bvh_samples\")\n",
    "\n",
    "gen = generate_5_bvh(\n",
    "    data_root=data_root,\n",
    "    ckpt_path=ckpt_path,\n",
    "    out_dir=out_dir,\n",
    "    device=\"cpu\",       # ✅ CPU로 강제\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d352ce-3435-4830-aece-5fd88e71e9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
